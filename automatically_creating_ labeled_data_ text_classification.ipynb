{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Data automatically\n",
    "\n",
    "For this code along we will evaluate each word and calculate the text value! We'll use the various NLP tools we learned about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import string\n",
    "import unicodedata\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import re\n",
    "from pyspark.conf import SparkConf\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"nlp\")\\\n",
    "    .config(\"spark.executor.memory\", \"32g\")\\\n",
    "    .config(\"spark.driver.memory\", \"32g\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"16g\")\\\n",
    "    .config(\"spark.debug.maxToStringFields\",\"200\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=spark.read.csv(\"./data/7_july_1.csv\", inferSchema=True, encoding = 'utf8', header=True).select('_c0','_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets.selectExpr(\"_c0 as date\", \"_c1 as text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|               date|                text|\n",
      "+-------------------+--------------------+\n",
      "|2018-07-07 18:52:11|b'Intensity build...|\n",
      "|2018-07-07 18:52:11|b'Never die attit...|\n",
      "|2018-07-07 18:52:11|b'RT @FIFAWorldCu...|\n",
      "|2018-07-07 18:52:11|b'RT @HNS_CFF: \\x...|\n",
      "|2018-07-07 18:52:10|b'RT @ThunderStur...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove https in the text\n",
    "def remove_https(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text\n",
    "udfhttps=udf(lambda text: remove_https(text), StringType())\n",
    "tweets = tweets.withColumn(\"text\", udfhttps(tweets[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(text):    \n",
    "    text = unicodedata.normalize('NFKD', str(text))\n",
    "    text = text.replace(r'\\n', '')\n",
    "    text = ' '.join(text.split())\n",
    "    replace_punctuation = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "    text = text.translate(replace_punctuation)\n",
    "    text = text.encode('ASCII', 'ignore')\n",
    "    text = text.decode('unicode_escape')\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text\n",
    "\n",
    "udfNormalizeData=udf(lambda text: normalizeData(text), StringType())\n",
    "\n",
    "tweets = tweets.withColumn(\"text\", udfNormalizeData(tweets[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATIN_1_CHARS = (\n",
    "    (' xe2 x80 x99', \"'\"),\n",
    "    (' xc3 xa9', 'e'),\n",
    "    (' xe2 x80 x90', '-'),\n",
    "    (' xe2 x80 x91', '-'),\n",
    "    (' xe2 x80 x92', '-'),\n",
    "    (' xe2 x80 x93', '-'),\n",
    "    (' xe2 x80 x94', '-'),\n",
    "    (' xe2 x80 x94', '-'),\n",
    "    (' xe2 x80 x98', \"'\"),\n",
    "    (' xe2 x80 x9b', \"'\"),\n",
    "    (' xe2 x80 x9c', '\"'),\n",
    "    (' xe2 x80 x9c', '\"'),\n",
    "    (' xe2 x80 x9d', '\"'),\n",
    "    (' xe2 x80 x9e', '\"'),\n",
    "    (' xe2 x80 x9f', '\"'),\n",
    "    #(' xe2 x80 xa6', '...'),\n",
    "    (' xe2 x80 xa6', ''),\n",
    "    (' xe2 x80 xb2', \"'\"),\n",
    "    (' xe2 x80 xb3', \"'\"),\n",
    "    (' xe2 x80 xb4', \"'\"),\n",
    "    (' xe2 x80 xb5', \"'\"),\n",
    "    (' xe2 x80 xb6', \"'\"),\n",
    "    (' xe2 x80 xb7', \"'\"),\n",
    "    (' xe2 x81 xba', \"+\"),\n",
    "    (' xe2 x81 xbb', \"-\"),\n",
    "    (' xe2 x81 xbc', \"=\"),\n",
    "    (' xe2 x81 xbd', \"(\"),\n",
    "    (' xe2 x81 xbe', \")\"),\n",
    "    (' xe2 x80 xa7', \".\"),\n",
    "    ('.', \" \"),\n",
    ")\n",
    "def clean_latin1(data):\n",
    "    for _hex, _char in LATIN_1_CHARS:\n",
    "        data = data.replace(_hex, _char)\n",
    "    return data\n",
    "\n",
    "udfDecoding=udf(lambda text: clean_latin1(text), StringType())\n",
    "tweets = tweets.withColumn(\"text\", udfDecoding(tweets[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = tweets.schema.fields\n",
    "stringFields = filter(lambda f: isinstance(f.dataType, StringType), fields)\n",
    "nonStringFields = map(lambda f: col(f.name), filter(lambda f: not isinstance(f.dataType, StringType), fields))\n",
    "stringFieldsTransformed = map(lambda f: lower(col(f.name)), stringFields) \n",
    "allFields = [*nonStringFields, *stringFieldsTransformed]\n",
    "\n",
    "tweets = tweets.select(allFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.select(col(\"date\").alias(\"date\"),col(\"lower(text)\").alias(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(minTokenLength= 3, inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tweets = regexTokenizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_\")\n",
    "tweets_ = remover.transform(tweets)\n",
    "tweets=tweets_.drop(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/emoji.txt', 'r') as f:\n",
    "    first_list=f.read().strip().splitlines()\n",
    "\n",
    "second_list = []\n",
    "for item in first_list:\n",
    "    x = item.split('\\\\')\n",
    "    second_list.append(x)\n",
    "\n",
    "third_list = []\n",
    "for item in second_list:\n",
    "    new_list = []\n",
    "    for e in item:\n",
    "        if e not in (''):\n",
    "            new_list.append(e)\n",
    "    third_list.append(new_list)\n",
    "    \n",
    "fourth_list = [item for sublist in third_list for item in sublist]\n",
    "\n",
    "emojies = []\n",
    "for word in fourth_list:   #for each word in line.split()\n",
    "    if word not in emojies:    #if a word isn't in line.split            \n",
    "        emojies.append(word)\n",
    "        \n",
    "remover = StopWordsRemover(inputCol=\"words_\", outputCol=\"words\", stopWords=emojies)\n",
    "tweets_ = remover.transform(tweets)\n",
    "\n",
    "tweets=tweets_.drop(\"words_\")\n",
    "tweets=tweets.selectExpr(\"date as date\", \"text as text\", \"words as words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|[intensity, building, sochi, ruscro]                                                                               |\n",
      "|[never, die, attitude, like, places, wada, ama, pls, partwe, need, clean, honest, athletes, worldcup, ruscro, cuak]|\n",
      "|[fifaworldcup, fantastic, goal, xbaa, strong, response, ruscro, worldcup]                                          |\n",
      "|[hns, cff, xball, play, sochi, players, take, break, beproud, croatia, flamingpride, ruscro, family, worldcup]     |\n",
      "|[thundersturm, caption, needed, ruscro, worldcup]                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.select('words').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model to find the weight of each words\n",
    "### Sumation of the words' weights and evaluate the text score \n",
    "\n",
    "** for doing this we use a concepts in the following article to calculate the value of each text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
