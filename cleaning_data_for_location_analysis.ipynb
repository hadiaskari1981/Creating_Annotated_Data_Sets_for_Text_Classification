{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This file clean the tweets and make it ready for analysis based on location**\n",
    "\n",
    "** It saves the whole process in a json file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_file import DOWNLOAD_ROOT, DATASETS_PATH, DOWNLOAD_URL, TWEETS_PATH\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import string\n",
    "import unicodedata\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import re\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "import string\n",
    "import csv\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "from config_file import DOWNLOAD_ROOT, DATASETS_PATH, DOWNLOAD_URL, TWEETS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").\\\n",
    "        appName(\"nlp\").config(\"spark.executor.memory\", \"32g\").\\\n",
    "        config(\"spark.driver.memory\", \"32g\").\\\n",
    "        config(\"spark.memory.offHeap.enabled\",True).\\\n",
    "        config(\"spark.memory.offHeap.size\",\"16g\").\\\n",
    "        config(\"spark.debug.maxToStringFields\",\"200\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    extract_tar_file(tgz_name = \"datasets.tar.gz\")\n",
    "    tweets = load_data(TWEETS_PATH, '*.csv')\n",
    "    tweets = tweets.selectExpr( \"_c1 as text\" , \"_c12 as location\")\n",
    "    latitude_longitude= load_data(DATASETS_PATH, 'country_capitals.csv')\n",
    "    latitude_longitude= latitude_longitude.select('countryName','capitalLatitude','capitalLongitude')\n",
    "    \n",
    "    udfhttps=udf(lambda text: remove_https(text), StringType())\n",
    "    tweets = tweets.withColumn(\"text\", udfhttps(tweets[\"text\"]))\n",
    "    split_col = split(tweets['location'], ':')\n",
    "    tweets = tweets.withColumn('location_name', split_col.getItem(1))\n",
    "    \n",
    "    udfNormalizeData=udf(lambda text: normalizeData(text), StringType())\n",
    "    tweets = tweets.withColumn(\"text\", udfNormalizeData(tweets[\"text\"]))\n",
    "    tweets = tweets.withColumn(\"location_name\", udfNormalizeData(tweets[\"location_name\"]))\n",
    "    \n",
    "    udfDecoding=udf(lambda text: clean_latin(text), StringType())\n",
    "    tweets = tweets.withColumn(\"text\", udfDecoding(tweets[\"text\"]))\n",
    "    tweets= lower_words(tweets)\n",
    "    tweet= tweets.where(tweets.location.isNotNull())\n",
    "    tweets = tweets.drop('location')\n",
    "    tweets= regexTokenizer_StopWordsRemover(tweets, 'text', 'words_','words')\n",
    "    tweets = remove_emoji(tweets, \"words\",\"words_\")\n",
    "\n",
    "    regexTokenizer = RegexTokenizer(minTokenLength= 3, inputCol=\"location_name\",outputCol=\"location_name_token\", pattern=\"\\\\W\")\n",
    "\n",
    "    tweets = regexTokenizer.transform(tweets)\n",
    "    tweets= tweets.where(size(col(\"location_name_token\")) >0)\n",
    "    join_udf = udf(lambda x: \" \".join(x))\n",
    "    tweets=tweets.withColumn(\"location_name_token\", join_udf(col(\"location_name_token\")))\n",
    "    tweets = tweets.drop('text', 'location_name')\n",
    "    tweets=tweets.selectExpr('location_name_token as location', 'words as words' )\n",
    "    tweets = change_city_with_country(tweets)\n",
    "    tweets.repartition(1).write.json(os.path.join(DATASETS_PATH,'tweets_location'))\n",
    "    tweets_location = spark.read.json(os.path.join(DATASETS_PATH, \"tweets_location\", \"tweets_location.json\"))\n",
    "    df_countries_happiness = avg_happiness_of_words(tweets_location, 'words')\n",
    "    df.repartition(1).write.json(os.path.join(DATASETS_PATH,'countries_happiness'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|location|               words|\n",
      "+--------+--------------------+\n",
      "|  london|[game, last, nigh...|\n",
      "|    none|[fifaworldcup, ng...|\n",
      "+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
