{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from config_file import DATASETS_PATH, DOWNLOAD_ROOT, DOWNLOAD_URL, TWEETS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_ID = \"sentiment_analysis_ML\"\n",
    "\n",
    "if not os.path.isdir(\"images\"):\n",
    "    os.makedirs(\"images\")\n",
    "        \n",
    "if not os.path.isdir(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "MODEL_PATH = os.path.join('models', PROJECT_ID)\n",
    "        \n",
    "IMAGE_PATH = os.path.join(\"images\", PROJECT_ID)\n",
    "\n",
    "if not os.path.isdir(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "if not os.path.isdir(IMAGE_PATH):\n",
    "        os.makedirs(IMAGE_PATH)\n",
    "        \n",
    "MODELS_STORE_PATH = os.path.join(PROJECT_ROOT_DIR, MODEL_PATH)\n",
    "IMAGES_STORE_PATH = os.path.join(PROJECT_ROOT_DIR, IMAGE_PATH)\n",
    "\n",
    "def save_model(model_name, model):\n",
    "    path = os.path.join(MODELS_STORE_PATH, model_name)\n",
    "    model.save(path)\n",
    "    \n",
    "def load_model(model_path, model):\n",
    "    return model.load(model_path)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_STORE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import string\n",
    "import unicodedata\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, CountVectorizer, Word2Vec, IDF\n",
    "from pyspark.ml.feature import  IndexToString\n",
    "from pyspark.ml.feature import StringIndexer, StopWordsRemover, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.classification import LinearSVC, NaiveBayes, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVCModel, NaiveBayesModel, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vector\n",
    "import re\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "import string\n",
    "import csv\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[3]\")\\\n",
    "    .appName(\"nlp\")\\\n",
    "    .config(\"spark.executor.memory\", \"128g\")\\\n",
    "    .config(\"spark.driver.memory\", \"128g\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"64g\")\\\n",
    "    .config(\"spark.debug.maxToStringFields\",\"256\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tar_file(tgz_name= \"training_data.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(DATASETS_PATH, 'training_data/training_text_classification.csv')\n",
    "training_df = spark.read.format(\"libsvm\")\\\n",
    "        .csv(csv_path, inferSchema=True, encoding = 'ISO-8859-1', header=False)\\\n",
    "        .selectExpr('_c0 as label', '_c5 as text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfhttps=udf(lambda text: remove_https(text), StringType())\n",
    "udfNormalizeData=udf(lambda text: normalizeData(text), StringType())\n",
    "udfDecoding=udf(lambda text: clean_latin(text), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df= training_df.withColumn(\"text\", udfhttps(training_df[\"text\"]))\n",
    "training_df = training_df.withColumn(\"text\", udfNormalizeData(training_df[\"text\"]))\n",
    "training_df= training_df.withColumn(\"text\", udfDecoding(training_df[\"text\"]))\n",
    "training_df = lower_words(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    0| switchfoot   aww...|\n",
      "|    0|is upset that he ...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_df.withColumn('length',length(training_df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_df.withColumn(\"label\", \\\n",
    "              when(training_df[\"label\"] == 4, 1).otherwise(training_df[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.where((col(\"label\") == 0.0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.where((col(\"label\") == 1.0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|label|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|    0| switchfoot   aww...|    89|\n",
      "|    0|is upset that he ...|   110|\n",
      "+-----+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for Logistic Regression and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='features')\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "#featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "#labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "\n",
    "data_prep_pipe = Pipeline(stages=[tokenizer,stopremove, count_vec])\n",
    "cleaner_lr = data_prep_pipe.fit(training_df)\n",
    "data_lr = cleaner_lr.transform(training_df)\n",
    "\n",
    "data_lr = data_lr.select(['label','features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (10% held out for testing)\n",
    "(trainingData, testData) = data_lr.randomSplit([0.9, 0.1])\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(family=\"multinomial\")\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model_lr = mlr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(\"logistic_regression\", model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/sentiment_analysis_ML/logistic_regression\n"
     ]
    }
   ],
   "source": [
    "# path = os.path.join(MODELS_STORE_PATH, \"logistic_regression\")\n",
    "# saved_model_lr = load_model(path, LogisticRegressionModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting sentiment wiht logistic regression is: 0.6467255384703975\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "# predictions_lr = saved_model_lr.transform(testData)\n",
    "# acc = acc_eval.evaluate(predictions_lr)\n",
    "# print(\"Accuracy of model at predicting sentiment wiht logistic regression is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting sentiment wiht logistic regression is: 0.750852408946616\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions_lr = model_lr.transform(testData)\n",
    "acc = acc_eval.evaluate(predictions_lr)\n",
    "print(\"Accuracy of model at predicting sentiment wiht logistic regression is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (10% held out for testing)\n",
    "(trainingData, testData) = data_lr.randomSplit([0.9, 0.1])\n",
    "# Train model.\n",
    "svm = LinearSVC(maxIter= 10)\n",
    "model_svm = svm.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(\"svm\", model_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting sentiment with SVM is: 0.7707139719967024\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions_svm = model_svm.transform(testData)\n",
    "acc = acc_eval.evaluate(predictions_svm)\n",
    "print(\"Accuracy of model at predicting sentiment with SVM is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')\n",
    "\n",
    "data_prep_pipe = Pipeline(stages=[tokenizer,stopremove, count_vec, idf, clean_up])\n",
    "cleaner_nb = data_prep_pipe.fit(training_df)\n",
    "data_np = cleaner_nb.transform(training_df)\n",
    "data_np = data_np.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (10% held out for testing)\n",
    "(trainingData,testData) = data_np.randomSplit([0.9,0.1], seed=42)\n",
    "\n",
    "# Train the Model\n",
    "nb = NaiveBayes()\n",
    "model_nb = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\"naive_bayes\", model_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting sentiment with Naive Bayes is: 0.7422832448858636\n"
     ]
    }
   ],
   "source": [
    "# Make Prediction\n",
    "predictions_nb =model_nb.transform(testData)\n",
    "\n",
    "acc = acc_eval.evaluate(predictions_nb)\n",
    "print(\"Accuracy of model at predicting sentiment with Naive Bayes is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "word2vec = Word2Vec(inputCol='stop_tokens', outputCol = 'words_vector') \n",
    "clean_up = VectorAssembler(inputCols=['words_vector','length'],outputCol='features')\n",
    "\n",
    "data_prep_pipe = Pipeline(stages=[tokenizer,stopremove,word2vec, clean_up])\n",
    "cleaner_mlp = data_prep_pipe.fit(training_df)\n",
    "data_mlp = cleaner_mlp.transform(training_df)\n",
    "data_mlp = data_mlp.select(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, testing) = data_mlp.randomSplit([0.9,0.1])\n",
    "\n",
    "layers = [101, 25, 2]\n",
    "mlp = MultilayerPerceptronClassifier(layers= layers)\n",
    "model_mlp = mlp.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting sentiment was using MLP: 0.6715418982239492\n"
     ]
    }
   ],
   "source": [
    "predictions_mlp = model_mlp.transform(testing)\n",
    "acc = acc_eval.evaluate(predictions_mlp)\n",
    "print(\"Accuracy of model at predicting sentiment using MLP is: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessed Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tar_file(tgz_name=\"preproccessed_tweets_date.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_avg = spark.read.json('./datasets/sentiment_analysis/tweets_avg_happiness.json')\n",
    "h_words = spark.read.json('./datasets/sentiment_analysis/tweets_classification.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|                date|happiness_avg|\n",
      "+--------------------+-------------+\n",
      "|2018-06-15T11:23:...|        6.595|\n",
      "|2018-06-15T11:23:...|        6.454|\n",
      "+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h_avg.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                date|     happiness_words|\n",
      "+--------------------+--------------------+\n",
      "|2018-06-15T11:23:...|[good, work, get,...|\n",
      "|2018-06-15T11:23:...|[next, peeps, enj...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h_words.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some extre cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_words = h_words.withColumn('date' , regexp_replace('date', \"[\\.tT]\", ' '))\n",
    "h_avg = h_avg.withColumn('date' , regexp_replace('date', \"[\\.tT]\", ' '))\n",
    "\n",
    "split_col = pyspark.sql.functions.split(h_words['date'], ' ')\n",
    "h_words = h_words.withColumn('time', split_col.getItem(1))\n",
    "h_words = h_words.withColumn('date', split_col.getItem(0))\n",
    "h_words = h_words.withColumn('date' , (concat(col(\"date\"), lit(\" \"), col(\"time\")))).drop('time')\n",
    "\n",
    "split_col = pyspark.sql.functions.split(h_avg['date'], ' ')\n",
    "h_avg = h_avg.withColumn('time', split_col.getItem(1))\n",
    "h_avg = h_avg.withColumn('date', split_col.getItem(0))\n",
    "h_avg = h_avg.withColumn('date' , (concat(col(\"date\"), lit(\" \"), col(\"time\")))).drop('time')\n",
    "\n",
    "h_words = h_words.createOrReplaceTempView('h_words')\n",
    "h_words = spark.sql('select row_number() over (order by \"date\") as num, * from h_words')\n",
    "\n",
    "h_avg.createOrReplaceTempView('h_avg')\n",
    "h_avg = spark.sql('select row_number() over (order by \"date\") as num, * from h_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "|num|               date|     happiness_words|\n",
      "+---+-------------------+--------------------+\n",
      "|  1|2018-06-15 11:23:49|[good, work, get,...|\n",
      "|  2|2018-06-15 11:23:49|[next, peeps, enj...|\n",
      "+---+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h_words.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining two dataframe (h_words, h_avg)\n",
    "\n",
    "h_avg_words = h_words.join(h_avg, on='num', how = 'inner')\\\n",
    "                                    .drop('num').select(col('h_words.date').alias('date'),\\\n",
    "                                    col('happiness_words').alias('happiness_words'),\\\n",
    "                                    col('happiness_avg').alias('happiness_avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|               date|     happiness_words|happiness_avg|\n",
      "+-------------------+--------------------+-------------+\n",
      "|2018-06-15 11:23:49|[good, work, get,...|        6.595|\n",
      "|2018-06-15 11:23:49|[next, peeps, enj...|        6.454|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h_avg_words.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+--------------------+\n",
      "|               date|     happiness_words|happiness_avg|                text|\n",
      "+-------------------+--------------------+-------------+--------------------+\n",
      "|2018-06-15 11:23:49|[good, work, get,...|        6.595|good work get wor...|\n",
      "|2018-06-15 11:23:49|[next, peeps, enj...|        6.454|next peeps enjoy ...|\n",
      "+-------------------+--------------------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# removing the instances with less than four words\n",
    "h_avg_words = h_avg_words.where(size(col(\"happiness_words\")) >= 4)\n",
    "h_avg_words = h_avg_words.withColumn(\"text\", concat_ws(\" \", \"happiness_words\"))\n",
    "h_avg_words.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_happiness = cleaner_lr.transform(h_avg_words)\n",
    "data_happainess = data_happiness.select('date','happiness_avg','features')\n",
    "happiness_prediction =model_lr.transform(data_happainess)\n",
    "happiness_prediction.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165084"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_prediction.where((col(\"happiness_avg\") > 6.0) & (col(\"prediction\") == 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34617"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_prediction.where((col(\"happiness_avg\") > 6.0) & (col(\"prediction\") == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_prediction.where((col(\"happiness_avg\") < 4.0) & (col(\"prediction\") == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1963"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_prediction.where((col(\"happiness_avg\") < 4.0) & (col(\"prediction\") == 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127640"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_prediction.where((col(\"happiness_avg\")>= 4.0) & (col(\"happiness_avg\")<= 6.0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330449"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_prediction.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202809"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "330449-127640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = h_avg_words.where((col(\"happiness_avg\")< 4.0) | (col(\"happiness_avg\")> 6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202809"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = h_avg_words.withColumn(\"label\", F.when(h_avg_words[\"happiness_avg\"]<4, 0).\\\n",
    "                            when(h_avg_words[\"happiness_avg\"]>6, 1).otherwise('Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure tweets_label_distribution\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts().plot(kind='bar' , )\n",
    "save_fig(\"tweets_label_distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>happiness_words</th>\n",
       "      <th>happiness_avg</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-15 11:23:49</td>\n",
       "      <td>[good, work, get, worldcup]</td>\n",
       "      <td>6.595</td>\n",
       "      <td>good work get worldcup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-15 11:23:49</td>\n",
       "      <td>[next, peeps, enjoy, games, please, spare, tim...</td>\n",
       "      <td>6.454</td>\n",
       "      <td>next peeps enjoy games please spare time preci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                    happiness_words  \\\n",
       "0  2018-06-15 11:23:49                        [good, work, get, worldcup]   \n",
       "1  2018-06-15 11:23:49  [next, peeps, enjoy, games, please, spare, tim...   \n",
       "\n",
       "   happiness_avg                                               text  \n",
       "0          6.595                             good work get worldcup  \n",
       "1          6.454  next peeps enjoy games please spare time preci...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haw_pandas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
