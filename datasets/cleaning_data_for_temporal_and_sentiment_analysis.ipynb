{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This file is for cleaning the tweets and prepare it for tmporal analysis and sentiment analysis using ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use cleaning process if needed\n",
    "from utils import *\n",
    "from config_file import DOWNLOAD_ROOT, DATASETS_PATH, DOWNLOAD_URL, TWEETS_PATH\n",
    "\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import string\n",
    "import unicodedata\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import re\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "import string\n",
    "import csv\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_ID = \"happiness_over_countries\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", PROJECT_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"nlp\")\\\n",
    "    .config(\"spark.executor.memory\", \"32g\")\\\n",
    "    .config(\"spark.driver.memory\", \"32g\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"16g\")\\\n",
    "    .config(\"spark.debug.maxToStringFields\",\"200\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    extract_tar_file(tgz_name = \"datasets.tar.gz\")\n",
    "    for i in range(1,36):\n",
    "        filename = str(i)+'.csv'\n",
    "        tweets = load_data(TWEETS_PATH, filename)\n",
    "        tweets = tweets.selectExpr(\"_c0 as date\", \"_c1 as text\")\n",
    "        tweets = tweets.withColumn(\"text\", udfhttps(tweets[\"text\"]))\n",
    "        tweets = tweets.withColumn(\"text\", udfNormalizeData(tweets[\"text\"]))\n",
    "        tweets = tweets.withColumn(\"text\", udfDecoding(tweets[\"text\"]))\n",
    "        tweets= lower_words(tweets)\n",
    "        tweets= regexTokenizer_StopWordsRemover(tweets, 'text', 'words_','words')\n",
    "        tweets = remove_emoji(tweets, \"words\",\"words_\")\n",
    "        #tweets.show(2)\n",
    "        if i == 1:\n",
    "            tweets = avg_happiness_of_words(tweets, 'words')\n",
    "            tweets_final = tweets\n",
    "\n",
    "        else:\n",
    "            tweets = avg_happiness_of_words(tweets, 'words')\n",
    "            tweets_final = tweets_final.union(tweets)\n",
    "            \n",
    "        tweets_final.write.json(os.path.join(DATASETS_PATH,'sentiment_analysis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
